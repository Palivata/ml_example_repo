project_name: 'HW1-modeling'
experiment_name: 'baseline'
num_classes: 17
n_epochs: 10
accelerator: 'gpu'
device: 0
monitor_metric: 'val_f1'
monitor_mode: 'max'
data_path: '/src/data'
batch_size: 48
num_iterations: 1000
n_workers: 4
train_size: 0.8
width: &width 224
height: &height 224
seed: 42

model_kwargs:
  model_name: 'efficientnet_b3'
  pretrained: true

optimizer: 'torch.optim.AdamW'
optimizer_kwargs:
  lr: 1e-4
  weight_decay: 1e-5

scheduler: 'torch.optim.lr_scheduler.CosineAnnealingLR'
scheduler_kwargs:
  T_max: 10
  eta_min: 1e-5

losses:
  - name: 'bce'
    weight: 1.0
    loss_fn: 'torch.nn.BCEWithLogitsLoss'
    loss_kwargs: {}

augmentations:
  train:
    Compose:
      color_jitter:
        p: 0.1
      random_scaling:
        p: 0.1
        angle_low: 0
        angle_max: 45
        scale_low: 1
        scale_max: 1.3
      random_gaussian_blur:
        p: 0.1
        kernel_low: 7
        kernel_max: 15
      discrete_flip_rotate:
        p: 1
      resize:
        p: 1
        height: *height
        width: *width
        resize_method: bicubic
  test:
    Compose:
      resize:
        p: 1
        height: *height
        width: *width
        resize_method: bicubic
