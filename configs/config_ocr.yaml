project_name: 'HW2-modeling'
experiment_name: 'ocr_baseline'
experiment: 'ocr'
num_classes: 11
n_epochs: 10
accelerator: 'gpu'
device: 0
monitor_metric: 'valid_ctc_loss'
monitor_mode: 'min'
data_path: '/src/data'
batch_size: 32
num_iterations: 500
n_workers: 16
train_size: 0.8
width: &width 416
height: &height 96
seed: 42

model_kwargs:
  model_name: 'efficientnet_b0'
  pretrained: true
  rnn_features_num: 48
  rnn_hidden_size: 64
  rnn_dropout: 0.1
  rnn_bidirectional: true
  num_classes: 11

optimizer: 'torch.optim.AdamW'
optimizer_kwargs:
  lr: 1e-3
  weight_decay: 1e-5

scheduler: 'torch.optim.lr_scheduler.ReduceLROnPlateau'
scheduler_kwargs:
  mode: 'min'
  factor: 0.5
  patience: 20
  min_lr: 1e-5

losses:
  - name: 'ctc'
    weight: 1.0
    loss_fn: 'torch.nn.CTCLoss'
    loss_kwargs: {}

augmentations:
  train:
    Compose:

      ocr_perspective_crop:
        p: 0.8
        width_ratio: 0.04
        height_ratio: 0.08

      x_scale:
        p: 0.8
        scale_min: 0.8
        scale_max: 1.2
      ocr_resize:
        p: 1
        target_height: *height
        target_width: *width
        mode: random
      color_jitter:
        p: 0.1
      random_scaling:
        p: 0.1
        angle_low: 0
        angle_max: 45
        scale_low: 1
        scale_max: 1.3
      random_gaussian_blur:
        p: 0.1
        kernel_low: 7
        kernel_max: 15
      text_encoding:
        p: 1
        vocab: '0123456789'
        target_text_size: 13
  test:
    Compose:
      ocr_resize:
        p: 1
        target_height: *height
        target_width: *width
        mode: left

      text_encoding:
        p: 1
        vocab: '0123456789'
        target_text_size: 13
